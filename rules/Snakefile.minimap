from Bio import SeqIO

# clustering parameters
WINDOW_SIZE = config.get('window_size', 2_000)
OVERLAP = config.get('window_overlap', int(WINDOW_SIZE/2))
WINDOW_SPACING = WINDOW_SIZE - OVERLAP
MIN_SIZE = config.get('minimum_size', 0)
MM_COV_FRAC = config.get('minimap_cov_frac', .8)

def get_cluster_mcl_files(wildcards):
    return [f'{output_dir}/windows/cluster/window.{window}/self.I{mcl_i}.mcl' 
            for window in get_windows()]

rule all:
    input: get_cluster_mcl_files

rule mcl:
    input: '{file_root}.abc'
    output: '{file_root}.I{mcl_i}.mcl'
    benchmark: '{file_root}.I{mcl_i}.mcl.time'
    threads: 19
    shell:
        "mcl {input} --abc -te {threads} -I {wildcards.mcl_i} -o {output} > {output}.log 2>&1"

rule merge_abc:
    input: lambda w: [f"{w.work_dir}/windows/cluster/window.{window}/self.abc" \
                      for window in get_windows()]
    output: "{work_dir}/mcl_all/all.abc"
    benchmark: "{work_dir}/mcl_all/all.abc.time"
    run:
        # load all abc files and merge (taking the best value for each pair
        mfrac_dict = {}
        for window_abc in input:
            window_mfrac = pandas.read_csv(window_abc, sep='\t', header=None, names=['query','hit','mfrac'])
            best_mfrac = window_mfrac.groupby(['hit','query']).agg(max)
            mfrac_dict.update(best_mfrac.mfrac.items())
            mfrac_dict.update(((h,q),m) for (q,h),m in best_mfrac.mfrac.items())

        # write out merged dict
        with open(str(output), 'wt') as abc_out:
            for (r1, r2), mfrac in mfrac_dict.items():
                abc_out.write(f"{r1}\t{r2}\t{mfrac:0.4f}\n")

rule window_abc:
    """
    Aggregates hits for query/hit pairs, filters for 80% coverage, and
    writes mfrac (= %ID * frac covered) to distance matrix (abc file)
    """
    input: '{output_dir}/windows/cluster/window.{window}/self.paf'
    output: '{output_dir}/windows/cluster/window.{window}/self.abc'
    benchmark: '{output_dir}/windows/cluster/window.{window}/self.abc.time'
    run:
        from jme.jupy_tools.hit_tables import agg_hit_table, PAF
        paf_file = str(input)
        if os.path.getsize(paf_file) > 0:
            agg_hit_table(paf_file, format=PAF) \
                .query(f'mlen >= {MM_COV_FRAC} * ((qlen + hlen) / 2)') \
                .set_index(['query', 'hit']) \
                .mfrac \
                .to_csv(str(output), sep='\t', header=None)
        else:
            # input is empty, touch the output
            with open(str(output), 'wt') as out_handle:
                pass

rule window_minimap:
    input: '{output_dir}/windows/reads/reads.{window}.fasta'
    output: '{output_dir}/windows/cluster/window.{window}/self.paf'
    benchmark: '{output_dir}/windows/cluster/window.{window}/self.paf.time'
    threads: 10
    shell:
        """
        minimap2 -x ava-ont -t 20 {input} {input} > {output} 2> {output}.log
        """

def get_windows():
    reads_dir = checkpoints.apply_fasta_size_window.get().output.fasta
    windows, = glob_wildcards(reads_dir + "/reads.{window}.fasta")
    return windows

checkpoint apply_fasta_size_window:
    """ divide all reads into windows by size

    for speed, this keeps an open handle to all output files. This may be 
        problamatic on some systems. Look into ulimt or file a bug report
        if you hit a limit of open file handles
            """
    input: ALL_FASTA
    output: 
        fasta=directory(f'{WORK_DIR}/windows/reads'),
        counts=f'{WORK_DIR}/windows/read_counts.txt',
        read_lens=f'{WORK_DIR}/all.reads.lengths.tsv'
    benchmark: f'{WORK_DIR}/all.reads.lengths.tsv.time'
    run:
        from jme.jupy_tools.utils import LogLogger
        llogger_read = LogLogger(multiplier=10)
        llogger_window = LogLogger(multiplier=2)
        handles = {}
        counts = {}
        read_lens = {}
        os.makedirs(str(output.fasta), exist_ok=True)
        for i, read in enumerate(SeqIO.parse(str(input), 'fasta')):
            l = len(read)
            if l < MIN_SIZE:
                continue

            read_lens[read.id] = l
            
            windows = []
            
            window_num = int(l / (WINDOW_SPACING))
            window_start = window_num * WINDOW_SPACING
            window_end = window_start + WINDOW_SIZE
            
            # shift down incase we'd be in the end of an earlier window
            while window_end > l + WINDOW_SPACING:
                window_num -= 1
                window_start = window_num * WINDOW_SPACING
                window_end = window_start + WINDOW_SIZE

            # loop over all windows this read is in
            while window_start <= l:
                window_end = window_start + WINDOW_SIZE
                window = f"{window_start}_{window_end}"

                counts[window] = counts.get(window, 0) + 1

                windows.append(window)

                try:
                    fasta_out = handles[window]
                except:
                    out_file = f"{output.fasta}/reads.{window}.fasta"
                    llogger_window.log("opnning file %d (%s) for window %s", (len(handles), out_file, window))
                    fasta_out = open(out_file, 'wt')
                    handles[window] = fasta_out

                fasta_out.write(read.format('fasta'))
            
                window_num += 1
                window_start = window_num * WINDOW_SPACING
            
            llogger_read.log("wrote read %d (%s) of len %d to window %s", (i, read.id, l, windows))


        print(f'Closing {len(handles)} handles')
        for handle in handles.values():
            handle.close()

        with open(str(output.read_lens), 'wt') as lens_out:
            for read, length in read_lens.items():
                lens_out.write(f"{read}\t{length}\n")

        with open(str(output.counts), 'wt') as counts_out:
            for window, count in counts.items():
                counts_out.write(f"{window}\t{count}\n")
            
            
